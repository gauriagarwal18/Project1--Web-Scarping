{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauriagarwal18/Project1--Web-Scarping/blob/master/web_scraping_git_topics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***SCRAPPING TOP REPOSITORIES FOR TOPICS ON GITHUB ***"
      ],
      "metadata": {
        "id": "V9oqJWBIqi2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WEB SCRAPING\n",
        "\n",
        "####-Introduction to web scrapping-: \n",
        "Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites for drawing conclusions from the data or for performing other analysis.\n",
        "\n",
        "####-Introduction about GitHub-:\n",
        "GitHub, Inc. is a provider of Internet hosting for software development and version control using Git. It offers the distributed version control and source code management functionality of Git\n",
        "\n",
        "link to github: https://github.com"
      ],
      "metadata": {
        "id": "pfkZxZyLTy4k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrC_noHogmjK"
      },
      "source": [
        "# Project Outline\n",
        "\n",
        "\n",
        "-we are going to scrape - https://github.com/topics website\n",
        "\n",
        "-we will get a list of topic and for each topic, we'll get topic title, topic page url,and topic description\n",
        "\n",
        "-for each topic we will get top 25 repositories in that topic from the topic page.\n",
        "\n",
        "-for each repository we will mention its name,username, stars,repo url\n",
        "\n",
        "-for each topic we will create a csv file in the following format-------\n",
        "```\n",
        "repo_name,username,stars,repo_url\n",
        "three.js,mrdoob,77700,https://github.com/mrdoob/three.js\n",
        "```\n",
        "-and then we will put all the csv files as an excel sheet in an excel file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing required libraries and importing them.\n",
        "\n",
        "###pandas for collecting data\n",
        "for collecting all the info in the form of a data frame and stroring it in excel file or in csv file as required\n",
        "\n",
        "### requests library to download web pages\n",
        "Inspect the website's HTML source and identify the right URLs to download.\n",
        "Download and save web pages locally using the requests library.\n",
        "Create a function to automate downloading for different topics/search queries.\n",
        "\n",
        "###BeautifulSoup to parse and extract information\n",
        "Parse and explore the structure of downloaded web pages using Beautiful soup.\n",
        "Use the right properties and methods to extract the required information.\n",
        "Create functions to extract from the page into lists and dictionaries.\n",
        "(Optional) Use a REST API to acquire additional information if required.\n",
        "create the csv file\n"
      ],
      "metadata": {
        "id": "ucX-lRiVm9MH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xF06Ieqgmjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c0842e6-70f2-4d16-a4c5-24f18bd72d99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "!pip install requests\n",
        "import requests\n",
        "!pip install beautifulsoup4\n",
        "from bs4 import BeautifulSoup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "id": "VB40a4N_gmjp"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function: scrape_topics\n",
        "for getting info about the topics in git i will make a function:\n",
        "\n",
        "####Parameters-\n",
        "\n",
        "*it will take the url of page and extract the info.\n",
        "\n",
        "####Use\n",
        "\n",
        "*it parse the html contents of web page and \n",
        "\n",
        "*extract topic name,url,dsescription\n",
        "\n",
        "*combine the information in a single dataframe\n",
        "\n",
        "\n",
        "####Return\n",
        "\n",
        "*dataframe containing the info about the topic"
      ],
      "metadata": {
        "id": "0Ry6UQyQo2n2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topics(url='https://github.com/topics'):\n",
        "\n",
        "  #extractting the html code of the website which we want to extract\n",
        "  response=requests.get(url)\n",
        "  page_content=response.text\n",
        "\n",
        "  #parsing the above requested website so we can draw useful info from it\n",
        "  soup=BeautifulSoup(page_content,'html.parser')\n",
        "  selection_class=\"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
        "  topic_title_tags=soup.find_all('p',{'class':selection_class})    \n",
        "\n",
        "  #topic discription tags\n",
        "  selection_class_des=\"f5 color-fg-muted mb-0 mt-1\"\n",
        "  topic_description_tags=soup.find_all('p',{'class':selection_class_des}) \n",
        "\n",
        "  #topics link tags\n",
        "  selection_class_link=\"no-underline flex-1 d-flex flex-column\"\n",
        "  topic_url_tags=soup.find_all('a',{'class':selection_class_link})\n",
        "\n",
        "  #getting topics titles ,topic tags ,topic urls\n",
        "  base_url=\"https://github.com\"\n",
        "  topic_titles=[tag.text for tag in topic_title_tags]\n",
        "  topic_description=[(tag.text).strip() for tag in topic_description_tags]\n",
        "  topic_url=[base_url+tag['href'] for tag in topic_url_tags]\n",
        "  topics_df=pd.DataFrame({'titles':topic_titles,'url':topic_url,'description':topic_description})\n",
        "\n",
        "  return topics_df"
      ],
      "metadata": {
        "id": "ZE3abmu2oqo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMkwBcNxgmj0"
      },
      "source": [
        "# for top repositores of each project\n",
        "\n",
        "Created two functions for the end-to-end process of downloading, parsing, and saving CSVs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##I have made two functions:\n",
        "###1st function: scrape_topics_repo\n",
        "\n",
        "####Parameter\n",
        "\n",
        "*it will take the topic url as a paremeter\n",
        "\n",
        "####Use\n",
        "\n",
        "*Collect the info, scrape the page of that particular topic\n",
        "\n",
        "*extract the  top repositories names,usernames,stars,repo url\n",
        "\n",
        "*save all the information in a dataframe.\n",
        "\n",
        "####Return\n",
        "\n",
        "*return a data frame which have all the information about top repositries of a project.\n"
      ],
      "metadata": {
        "id": "tuRnBEMBksDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topics_repos(url):\n",
        "\n",
        "  #use this base url for getting complete urls of each repositore\n",
        "  base_url=\"https://github.com\"\n",
        "\n",
        "  #reading the website in text format\n",
        "  response0=requests.get(url)\n",
        "  page_content0=response0.text\n",
        "  \n",
        "  #parsing the html content of website\n",
        "  soup0=BeautifulSoup(page_content0,'html.parser')\n",
        "  class_parent=\"f3 color-fg-muted text-normal lh-condensed\"\n",
        "\n",
        "  #finding the parent tag which contains user name,repo name and repo url\n",
        "  repo_parent_tags=soup0.find_all('h3',{'class':class_parent})\n",
        "  \n",
        "  #iterating one by one for each repository\n",
        "  repo_user=[]\n",
        "  repo_name=[]\n",
        "  repo_url=[]\n",
        "  for repo in repo_parent_tags:\n",
        "    a_tag=repo.find_all('a')\n",
        "    repo_user.append(a_tag[0].text.strip())\n",
        "    repo_name.append(a_tag[1].text.strip())\n",
        "    repo_url.append(base_url+a_tag[1]['href'])\n",
        "  \n",
        "\n",
        "  #for getting stars of each repository\n",
        "  class_stars=\"Counter js-social-count\"\n",
        "  star_tags=soup0.find_all(\"span\",{'class':class_stars})\n",
        "\n",
        "  repo_stars=[]\n",
        "  for tag in star_tags:\n",
        "    if(tag.text[-1]=='k'):\n",
        "      repo_stars.append(float(tag.text[:-1])*1000)\n",
        "    else:\n",
        "      repo_stars.append(float(tag.text))\n",
        "\n",
        "  #for making dictionary of info of all repositories\n",
        "  repo_dict={\"user\":repo_user,\"name\":repo_name,\"url\":repo_url,\"stars\":repo_stars}\n",
        "\n",
        "  #collecting all info in a dataframe\n",
        "  repo_df=pd.DataFrame(repo_dict)\n",
        "  return repo_df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bx2Ap3NNkv6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###2nd function: make_file\n",
        "\n",
        "####Parameter-\n",
        "\n",
        "*It will take url of github topic page link as a parameter\n",
        "\n",
        "####Use\n",
        "*it will traverse throughth the url column of topics data frame\n",
        "\n",
        "*one by one call the function and get the data of top repos\n",
        "\n",
        "*save the data as a separate sheet of excel file with name as its topic name\n",
        "\n",
        "####Return\n",
        "*none (it will not return anything)"
      ],
      "metadata": {
        "id": "vX7cMV46xhge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_file(url):\n",
        "\n",
        "  topics_df=scrape_topics(url)             #get the dataframe which have info about the topic\n",
        "  topics_df.to_excel('github_topics_top_repos.xlsx',sheet_name=\"git_topics\")\n",
        "  for name,url in zip(topics_df['titles'],topics_df['url']):\n",
        "    repos=scrape_topics_repos(url)\n",
        "    with pd.ExcelWriter('github_topics_top_repos.xlsx',mode='a') as writer:  # doctest: +SKIP\n",
        "      repos.to_excel(writer, sheet_name=name)\n"
      ],
      "metadata": {
        "id": "5NT65H8phdyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calling the function for creating the excel file\n",
        "def main():\n",
        "  url='https://github.com/topics' \n",
        "  make_file(url)                 #make as excel file with sheets each having info about top repositories of the repective topics\n"
      ],
      "metadata": {
        "id": "y_VRbrS4Ncm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calling our main function where link to our target page is saved.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "UucW8OZ8Ng1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Explaination of how all the functions are interacting with each other----\n",
        "\n",
        "*main function will be called at first.\n",
        "\n",
        "*main function consits the url of target page and it will call the make_file function to generate the excel file.\n",
        "\n",
        "*make_file will then call scrape_topics function to get the datarame with information about each topic.\n",
        "\n",
        "*Then it will iterate through the dataset and extract info about the top repositories of each project by calling scrape_topics_repo function.\n",
        "\n",
        "*and then the excel file is created with the required information\n"
      ],
      "metadata": {
        "id": "6UPi1xrft9Bn"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "name": "web_scraping_git_topics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}